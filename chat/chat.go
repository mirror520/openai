package chat

import (
	"encoding/json"

	"github.com/oklog/ulid/v2"
)

type ChatID ulid.ULID

func ParseID(id string) (ChatID, error) {
	chatID, err := ulid.Parse(id)
	if err != nil {
		return ChatID{}, err
	}
	return ChatID(chatID), nil
}

func (id ChatID) String() string {
	return ulid.ULID(id).String()
}

func (id *ChatID) MarshalJSON() ([]byte, error) {
	jsonStr := `"` + id.String() + `"`
	return []byte(jsonStr), nil
}

func (id *ChatID) UnmarshalJSON(data []byte) error {
	var s string
	if err := json.Unmarshal(data, &s); err != nil {
		return err
	}

	userID, err := ParseID(s)
	if err != nil {
		return err
	}

	*id = userID
	return nil
}

type Chat struct {
	ID       ChatID
	Model    string
	Messages []*Message
	*Options
}

func NewChat(model string, prompt string, opts *Options) *Chat {
	c := new(Chat)
	c.ID = ChatID(ulid.Make())
	c.Model = model

	c.Messages = []*Message{
		{
			Role:    System,
			Content: prompt,
		},
	}

	c.Options = opts

	return c
}

func (c *Chat) AddMessage(msg *Message) {
	c.Messages = append(c.Messages, msg)
}

func (c *Chat) Request() *Request {
	req := &Request{
		Model:    c.Model,
		Messages: c.Messages,
	}

	// clone options
	if c.Options != nil {
		req.Options = *c.Options
	}

	return req
}

type Options struct {

	// What sampling temperature to use, between 0 and 2.
	// Higher values like 0.8 will make the output more random,
	// while lower values like 0.2 will make it more focused and deterministic.
	//
	// We generally recommend altering this or top_p but not both.
	Temperature *float64 `json:"temperature,omitempty"`

	// An alternative to sampling with temperature, called nucleus sampling,
	// where the model considers the results of the tokens with top_p probability mass.
	// So 0.1 means only the tokens comprising the top 10% probability mass are considered.
	//
	// We generally recommend altering this or temperature but not both.
	TopP *float64 `json:"top_p,omitempty"`

	// How many chat completion choices to generate for each input message.
	N *int `json:"n,omitempty"`

	// If set, partial message deltas will be sent, like in ChatGPT.
	// Tokens will be sent as data-only server-sent events as they become available,
	// with the stream terminated by a data: [DONE] message
	Stream *bool `json:"stream,omitempty"`

	// Up to 4 sequences where the API will stop generating further tokens.
	Stop []string `json:"stop,omitempty"`

	// The maximum number of tokens to generate in the chat completion.
	//
	// The total length of input tokens and generated tokens is limited by the model's context length.
	MaxTokens *int `json:"max_tokens,omitempty"`

	// Number between -2.0 and 2.0.
	// Positive values penalize new tokens based on whether they appear in the text so far,
	// increasing the model's likelihood to talk about new topics.
	PresencePenalty *float64 `json:"presence_penalty,omitempty"`

	// Number between -2.0 and 2.0.
	// Positive values penalize new tokens based on their existing frequency in the text so far,
	// decreasing the model's likelihood to repeat the same line verbatim.
	FrequencyPenalty *float64 `json:"frequency_penalty,omitempty"`

	// Modify the likelihood of specified tokens appearing in the completion.
	//
	// Accepts a json object that maps tokens (specified by their token ID in the tokenizer)
	// to an associated bias value from -100 to 100. Mathematically,
	// the bias is added to the logits generated by the model prior to sampling.
	// The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection;
	// values like -100 or 100 should result in a ban or exclusive selection of the relevant token.
	LogitBias map[string]int `json:"logit_bias,omitempty"`

	// A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse.
	User *string `json:"user,omitempty"`
}

func (opts *Options) Update(newOpts *Options) error {
	if newOpts.Temperature != nil {
		opts.Temperature = newOpts.Temperature
	}

	if newOpts.TopP != nil {
		opts.TopP = newOpts.TopP
	}

	if newOpts.N != nil {
		opts.N = newOpts.N
	}

	if newOpts.Stream != nil {
		opts.Stream = newOpts.Stream
	}

	if newOpts.Stop != nil {
		opts.Stop = newOpts.Stop
	}

	if newOpts.MaxTokens != nil {
		opts.MaxTokens = newOpts.MaxTokens
	}

	if newOpts.PresencePenalty != nil {
		opts.PresencePenalty = newOpts.PresencePenalty
	}

	if newOpts.FrequencyPenalty != nil {
		opts.FrequencyPenalty = newOpts.FrequencyPenalty
	}

	if newOpts.LogitBias != nil {
		opts.LogitBias = newOpts.LogitBias
	}

	if newOpts.User != nil {
		opts.User = newOpts.User
	}

	return nil
}
